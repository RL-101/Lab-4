
# Some theory that helped grow some understanding
## <u> Why actor critic is important</u>
- it deals with continous action spaces
- it works by approximating a policy
- the agent has no idea how actions affect the environment thus we use a probability distribution
- the game is episodic - the agent has some starting state and ending state that causes the game to end
- we approximate value functions using neural networks
- estimation the value function is important beacuse it tells us the value of the current state and the value of any other state the agent may encounter
- optimal policy - policy that yeilds the best value funcyion for all states in the state space
- actor critic is thus used to approximate optimal policy

## <u> TD(Vanilla) Actor-critic </u>
[Implemented using ideas illustrated here!]( https://www.youtube.com/watch?v=K2qjAixgLqk&t=75s)
- has two neural networks
- one is used to directly approximate the agents' policy
> - the policy is a probabilty distribution over a set of action where we take state as input abd output a probability of selecting each action
- the other nueral network is used to approximate the value function
> it tells the actor how good each action is based on whether or not the resulting state is valuable
- has two cost functions
> - one for updating the $ \delta = R_t + \gamma V(s_{t+1}) - V(s_t) $
> - and the other for updating the critic $ \delta^{2} =  \delta\ln(\pi(A,s_t))$
- value of terminal state is identically zero

## <u>Algorithm Overview </u>
1. Initialize a deep neural network to model the actor and critic
2. Repeat for a large number of episodes
3. Reset score, environment, terminal flag
4. While the state is not terminal:
> - select an action based on the current state of the environment
> - take the action and receive the new state, reward and terminal flag from the environment
> - calculate delta and use it to update the actor and critic networks ($ \delta = R_t + \gamma V(s_{t+1})) - V(s_t)) $
> - set current state to the new state
> - increment the episode reward by the score

5. after all the episodes are finished plot the trend ad scores to look for evidence of learning 
> - there should be an overall increase in score over time
> - there will be a lot of oscillations because actor critic methods arent't really stable
> - overall trend should be upward


## <u>Implementation </u>
- we can use a single network for both the eactor and the critic
> - i.e have common input layers and two outputs, one for the actor and one for the critic
> - this has the benefit that we don't have to train two different networks to understand the environment 
- play about 2000 games, 1000 neurons in the first hidden layer and 500 units in te second hidden layer
- hard part is the actor
> - the actor models the policy, which is a probality distribution
> - the actor layer will have as many outputs as our actions and uses softmax activation because we are modelling probabilities 
> - when selcting actions we are going to be dealing with discrete action spaces, this is what is called a categorical distribution
> - we are going to use the tensor flow uderscore package for the categorical distribution
> - we are going to use probabilities generated by the actor layer to get this distribution which we can then sample
> - and use the built in log prob function for our cost function

